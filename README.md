# ğŸ›’ Retail Analytics Big Data System

A complete Lambda Architecture implementation for retail data analytics using Apache Spark, Kafka, Elasticsearch, and Streamlit.

## ğŸ“‹ Overview

This project implements a comprehensive retail analytics system that processes transactional data using both batch and real-time processing to provide:

- ğŸ“Š Real-time revenue and sales metrics
- ğŸ“ˆ 30-day demand forecasting
- ğŸ›’ Market basket analysis (association rules)
- ğŸ“¦ Inventory optimization
- ğŸ‘¥ Customer segmentation

## ğŸ—ï¸ Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Source â”‚ UCI Online Retail Dataset
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Kafka â”‚ â”‚ HDFS/S3 â”‚ â”‚ Batch â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚Processingâ”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
â”‚ â”‚ â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ Spark â”‚ â”‚ Spark â”‚ â”‚ ES â”‚
â”‚Streamingâ”‚ â”‚ Batch â”‚ â”‚ Serving â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
â”‚ â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Streamlit Dashboardâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

text

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.8+
- 8GB+ RAM
- 20GB+ disk space

### Installation

1. Clone repository
   git clone https://github.com/your-repo/retail-analytics-bigdata.git
   cd retail-analytics-bigdata

2. Run setup script
   chmod +x scripts/setup_environment.sh
   ./scripts/setup_environment.sh

3. Download dataset
   chmod +x scripts/download_data.sh
   ./scripts/download_data.sh

4. Initialize Elasticsearch
   python scripts/init_elasticsearch.py

text

### Running the System

1. Start infrastructure
   docker-compose up -d

2. Run batch processing jobs
   chmod +x scripts/run_batch_jobs.sh
   ./scripts/run_batch_jobs.sh

3. Start data simulator
   python src/ingestion/kafka_producer.py
   --csv data/raw/online_retail.csv
   --speed 10.0

4. Start streaming processor
   docker exec -it spark-master spark-submit
   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
   /opt/spark-apps/speed_layer/streaming_processor.py

5. Launch dashboard
   streamlit run dashboard/app.py

text

## ğŸ“Š Dashboard

Access the dashboard at: `http://localhost:8501`

### Features

1. **Real-time Overview**

   - Live revenue metrics
   - Top selling products
   - Country-wise distribution

2. **Demand Forecasting**

   - 30-day predictions per product
   - Confidence intervals
   - Export capabilities

3. **Market Basket Analysis**

   - Association rules
   - Product recommendations
   - Network visualization

4. **Inventory Optimization**
   - Safety stock calculations
   - Real-time alerts
   - Reorder point recommendations

## ğŸ› ï¸ Technology Stack

- **Data Ingestion:** Apache Kafka
- **Stream Processing:** Spark Structured Streaming
- **Batch Processing:** Apache Spark (PySpark)
- **Machine Learning:** Spark MLlib, Prophet
- **Storage:** HDFS, Elasticsearch
- **Visualization:** Streamlit, Plotly
- **Orchestration:** Docker Compose

## ğŸ“ Project Structure

retail-analytics-bigdata/
â”œâ”€â”€ config/ # Configuration files
â”œâ”€â”€ data/ # Data directory
â”œâ”€â”€ dashboard/ # Streamlit dashboard
â”œâ”€â”€ scripts/ # Setup scripts
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ batch_layer/ # Batch processing jobs
â”‚ â”œâ”€â”€ speed_layer/ # Streaming jobs
â”‚ â”œâ”€â”€ serving_layer/ # API & ES client
â”‚ â”œâ”€â”€ ingestion/ # Data producers
â”‚ â””â”€â”€ utils/ # Utilities
â”œâ”€â”€ tests/ # Unit tests
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

text

## ğŸ§ª Testing

Run all tests
python -m pytest tests/

Run specific test
python tests/test_batch_processing.py

text

## ğŸ“ˆ Performance

- **Batch Processing:** ~1M records in 30 minutes
- **Stream Processing:** ~10K records/second
- **Dashboard Latency:** <2 seconds
- **ES Query Time:** <100ms

## ğŸ”§ Configuration

Edit `config/config.yaml` to customize:

- Spark resources
- Kafka topics
- ES indices
- Algorithm parameters
- Alert thresholds

## ğŸ› Troubleshooting

### Services not starting

Check Docker logs
docker-compose logs -f [service-name]

Restart services
docker-compose restart

text

### Elasticsearch connection issues

Check ES health
curl http://localhost:9200/\_cluster/health

text

### Spark job failures

Check Spark UI
open http://localhost:8080

View executor logs
docker exec spark-master ls /opt/spark/work/

text

## ğŸ“š Documentation

- [Architecture Details](docs/ARCHITECTURE.md)
- [API Reference](docs/API.md)
- [Deployment Guide](docs/DEPLOYMENT.md)
- [Tuning Guide](docs/TUNING.md)

## ğŸ¤ Contributing

Contributions are welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md)

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE)

## ğŸ‘¥ Authors

- Your Name - [your-email@example.com]

## ğŸ™ Acknowledgments

- UCI Machine Learning Repository for the dataset
- Apache Spark community
- Streamlit team

## ğŸ“ Support

- Issues: [GitHub Issues](https://github.com/your-repo/issues)
- Email: support@example.com
